#!/usr/bin/env python
# coding: utf-8

# # Exploratory Data Analysis with Seaborn

# Task 1: Introduction and Importing the Data

# In[20]:


import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import time


# Task 2: Separate Target from Features
# 
# Now that the data set is in memory, we can explore the characteristics of its attributes and instances.
# 
# We will drop columns that cannot be used for analysis and classification.
# 
# Note that this does not constitute feature selection. We are dropping columns that have no bearing on the analysis. 
# we will be conducting, and will instead clutter our analysis. After producing descriptive statistics about the data, 
# we will separate the target from the features.
# 
# The target contains the diagnosis with binary class labels, M or B, for malignant and benign tumors respectively.

# In[21]:


data = pd.read_csv("/Users/Yifanlin/Desktop/coding/eda/data.csv")
data.head()


# In[22]:


col = data.columns
print(col)


# In[23]:


y = data.diagnosis
drop_cols=['Unnamed: 32','id','diagnosis']
x = data.drop(drop_cols,axis=1)
x.head()


# Task 3: Diagnosis Distribution Visualization
# 
# In this task, we will use Seaborn's countplot() method to visualize the target distributions.
# 
# We will also generate descriptive statistics about the features that summarize the central tendency, dispersion and shape of the data set's distribution.

# In[25]:


ax = sns.countplot(y,label = "count")
B,M = y.value_counts()
print('Number of Benign Tumors', B)
print('Number of Malignant Tumors', M)


# In[27]:


x.describe()


# Task 4: Visualizing Standardized Data with Seaborn
# 
# As the columns in the data set take on values of varying range, we need to standardize the data before proceeding with further analysis and visualization.
# 
# To begin feature analysis, we use Seaborn's violinplot() method. Violin plots are similar to box plots, except that they also show the probability density of the data at different values, usually smoothed by a kernel density estimator.

# In[32]:


data = x
data_std = (data - data.mean()) / data.std()
data = pd.concat([y, data_std.iloc[:, 0:10]], axis=1)
data = pd.melt(data,id_vars='diagnosis',
               var_name = 'features',
               value_name = 'value')
plt.figure(figsize = (10,10))
sns.violinplot(x='features',y='value', hue='diagnosis',data=data, split=True, inner='quart')
plt.xticks(rotation =45)


# Task 5: Violin Plots and Box Plots
# 
# We are using violin plots and box plots to identify features that best separate the data for classification.
# 
# Box plots are especially useful in identifying outliers in the data.
# 
# Using violin plots, we are also able to infer whether certain features are correlated.
# 
# To minimize clutter in our visualizations, we divide the features into three batches of ten features and produce separate plots for them.

# In[33]:


data = x
data_std = (data - data.mean()) / data.std()
data = pd.concat([y, data_std.iloc[:, 10:20]], axis=1)
data = pd.melt(data,id_vars='diagnosis',
               var_name = 'features',
               value_name = 'value')
plt.figure(figsize = (10,10))
sns.violinplot(x='features',y='value', hue='diagnosis',data=data, split=True, inner='quart')
plt.xticks(rotation =45)


# In[34]:


data = x
data_std = (data - data.mean()) / data.std()
data = pd.concat([y, data_std.iloc[:, 20:30]], axis=1)
data = pd.melt(data,id_vars='diagnosis',
               var_name = 'features',
               value_name = 'value')
plt.figure(figsize = (10,10))
sns.violinplot(x='features',y='value', hue='diagnosis',data=data, split=True, inner='quart')
plt.xticks(rotation =45)


# In[36]:


sns.boxplot(x='features',y='value', hue='diagnosis', data = data)
plt.xticks(rotation=45)


# Task 6: Using Joint Plots for Feature Comparison
# 
# Joint plots come in handy to illustrate the relationship between two features.
# 
# We will use seaborn's jointplot() method to draw a scatter plot with marginal histograms and kernel density fits. We can examine the relationship between any two features using the Pearson correlation coefficient of the regression through our scatter plot.

# In[40]:


sns.jointplot(x.loc[:,'concavity_worst'],
             x.loc[:,'concave points_worst'],
             kind='regg',
             color='#ce1414')


# Task 7: Observing the Distribution of Values and their Variance with Swarm Plots
# 
# We have learned that violin plots are a great tool for visualizing sparse distributions. As our data set contains close to 600 rows, we might want to simply display each point in the same visualization.
# 
# This need is satisfied by Seaborn's swarmplot() method. A swarm plot can be drawn on its own, but it is also a good complement to a box or violin plot in cases where you want to show all observations along with some representation of the underlying distribution.

# In[43]:


sns.set(style = 'whitegrid', palette = 'muted')
data = x
data_std = (data - data.mean()) / data.std()
data = pd.concat([y, data_std.iloc[:, 0:10]], axis=1)
data = pd.melt(data,id_vars='diagnosis',
               var_name = 'features',
               value_name = 'value')
plt.figure(figsize = (10,10))
sns.swarmplot(x='features',y='value', hue='diagnosis',data=data)
plt.xticks(rotation =45)


# In[44]:


sns.set(style = 'whitegrid', palette = 'muted')
data = x
data_std = (data - data.mean()) / data.std()
data = pd.concat([y, data_std.iloc[:, 10:20]], axis=1)
data = pd.melt(data,id_vars='diagnosis',
               var_name = 'features',
               value_name = 'value')
plt.figure(figsize = (10,10))
sns.swarmplot(x='features',y='value', hue='diagnosis',data=data)
plt.xticks(rotation =45)


# In[45]:


sns.set(style = 'whitegrid', palette = 'muted')
data = x
data_std = (data - data.mean()) / data.std()
data = pd.concat([y, data_std.iloc[:, 20:30]], axis=1)
data = pd.melt(data,id_vars='diagnosis',
               var_name = 'features',
               value_name = 'value')
plt.figure(figsize = (10,10))
sns.swarmplot(x='features',y='value', hue='diagnosis',data=data)
plt.xticks(rotation =45)


# Task 8: Observing all Pairwise Correlations
# 
# A good way to identify correlations between features is to visualize the correlation matrix as a heatmap.
# 
# We will make a note of the correlated features so that we can drop them from our data set before building a predictive model in the next project.

# In[49]:


f, ax = plt.subplots(figsize=(18,18))
sns.heatmap(x.corr(), annot=True, linewidth = .5, fmt='.1f',ax=ax)




